# üìù Publications 
(\* denotes equal contribution.)

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/4dnex.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**arXiv 2025**] [4DNeX: Feed-Forward 4D Generative Modeling Made Easy](https://4dnex.github.io/) \\
Zhaoxi Chen\*, **Tianqi Liu\***, Long Zhuo\*, Jiawei Ren, Zeng Tao, He Zhu, Fangzhou Hong, Liang Pan, Ziwei Liu. \\
[[Project page]](https://4dnex.github.io/)
[[Paper]](https://arxiv.org/abs/2508.13154)
[[Code]](https://github.com/3DTopia/4DNeX)
[[Dataset]](https://huggingface.co/datasets/3DTopia/4DNeX-10M)
[[Video]](https://www.youtube.com/watch?v=jaXNU1-0zgk)

4DNeX is a feed-forward framework that generates 4D (dynamic 3D) scene representations from a single image by adapting a video diffusion model. It produces high-quality dynamic point clouds and enables novel-view video synthesis.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/free4d.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**ICCV 2025**] [Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal Consistency](https://free4d.github.io/) \\
**Tianqi Liu\***, Zihao Huang\*, Zhaoxi Chen, Guangcong Wang, Shoukang Hu, Liao Shen, Huiqiang Sun, Zhiguo Cao, Wei Li, Ziwei Liu. \\
[[Project page]](https://free4d.github.io/)
[[Paper]](https://arxiv.org/pdf/2503.20785)
[[Code]](https://github.com/TQTQliu/Free4D)
[[Video]](https://youtu.be/GpHnoSczlhA)

Free4D is a tuning-free framework for 4D scene generation from a single image or text.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/mugs.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**ICCV 2025**] [MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction](https://arxiv.org/abs/2508.04297) \\
Yaopeng Lou, Liao Shen, **Tianqi Liu**, Jiaqi Li, Zihao Huang, Huiqiang Sun, Zhiguo Cao. \\
[[Paper]](https://arxiv.org/abs/2508.04297)
[[Code]](https://github.com/EuclidLou/MuGS)

MuGS is the first multi-baseline generalizable gaussian splatting method.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/dofgaussian.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**CVPR 2025**] [DoF-Gaussian: Controllable Depth-of-Field for 3D Gaussian Splatting](https://dof-gaussian.github.io/) \\
Liao Shen, **Tianqi Liu**, Huiqiang Sun, Jiaqi Li, Zhiguo Cao, Wei Li, Chen Change Loy. \\
[[Project page]](https://dof-gaussian.github.io/)
[[Paper]](https://arxiv.org/pdf/2503.00746)
[[Code]](https://github.com/leoShen917/DoF-Gaussian)

We introduce DoF-Gaussian, a controllable depth-of-field method for 3D-GS. We develop a lens-based imaging model based on geometric optics principles to control DoF effects. Our framework is customizable and supports various interactive applications.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/wildavatar.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**CVPR 2025**] [WildAvatar: Learning In-the-wild 3D Avatars from the Web](https://wildavatar.github.io/) \\
Zihao Huang, Shoukang Hu, Guangcong Wang, **Tianqi Liu**, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu. \\
[[Project page]](https://wildavatar.github.io/)
[[Paper]](https://arxiv.org/pdf/2407.02165)
[[Code]](https://github.com/wildavatar/WildAvatar_Toolbox)
[[Video]](https://youtu.be/nbK2n2rFJ0E)

We present WildAvatar, a web-scale in-the-wild video dataset for 3D avatar creation.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025 Highlight</div><img src='images/ch3depth.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**CVPR 2025 Highlight**] [CH<sub>3</sub>Depth: Efficient and Flexible Depth Foundation Model with Flow Matching](https://openaccess.thecvf.com/content/CVPR2025/papers/Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching_CVPR_2025_paper.pdf) \\
Jiaqi Li, Yiran Wang, Jinghong Zheng, Junrui Zhang, Liao Shen, **Tianqi Liu**, Zhiguo Cao. \\
[[Paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching_CVPR_2025_paper.pdf)
[[Code]](https://github.com/lijia7/CH3Depth)

CH‚ÇÉDepth is an efficient and flexible flow-matching-based depth estimation framework that achieves state-of-the-art zero-shot performance in accuracy, efficiency, and temporal consistency.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/MVSGaussian.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**ECCV 2024**] [MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo](https://mvsgaussian.github.io/) \\
**Tianqi Liu**, Guangcong Wang, Shoukang Hu, Liao Shen, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu.  \\
[[Project page]](https://mvsgaussian.github.io/)
[[Paper]](https://arxiv.org/abs/2405.12218)
[[Code]](https://github.com/TQTQliu/MVSGaussian)
[[Video]](https://youtu.be/4TxMQ9RnHMA)
[[‰∏≠ÊñáËß£ËØª]](https://mp.weixin.qq.com/s/Y9uXxNMgliV9p-ne_bGpEw)

MVSGaussian is a Gaussian-based method designed for efficient reconstruction of unseen scenes from sparse views in a single forward pass. It offers high-quality initialization for fast training and real-time rendering.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/dreammover.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
[**ECCV 2024**] [DreamMover: Leveraging the Prior of Diffusion Models for Image Interpolation with Large Motion](https://dreamm0ver.github.io/) \\
Liao Shen, **Tianqi Liu**, Huiqiang Sun, Xinyi Ye, Baopu Li, Jianming Zhang, Zhiguo Cao. \\
[[Project page]](https://dreamm0ver.github.io/)
[[Paper]](https://arxiv.org/abs/2409.09605)
[[Code]](https://github.com/leoShen917/DreamMover)

By leveraging the prior of diffusion models, DreamMover can generate intermediate images from image pairs with large motion while maintaining semantic consistency.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/gefu.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**CVPR 2024**] [Geometry-aware Reconstruction and Fusion-refined Rendering for Generalizable Neural Radiance Fields](https://gefucvpr24.github.io/) \\
**Tianqi Liu**, Xinyi Ye, Min Shi, Zihao Huang, Zhiyu Pan, Zhan Peng, Zhiguo Cao. \\
[[Project page]](https://gefucvpr24.github.io/)
[[Paper]](https://arxiv.org/abs/2404.17528)
[[Code]](https://github.com/TQTQliu/GeFu)
[[Video]](https://youtu.be/Z4RgnsKF3Gs)


We present GeFu, a generalizable NeRF method that synthesizes novel views from multi-view images in a single forward pass.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/RStab.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**CVPR 2024**] [3D Multi-frame Fusion for Video Stabilization](https://arxiv.org/pdf/2404.12887) \\
Zhan Peng, Xinyi Ye, Weiyue Zhao, **Tianqi Liu**, Huiqiang Sun, Baopu Li, Zhiguo Cao. \\
[[Paper]](https://arxiv.org/pdf/2404.12887)
[[Code]](https://github.com/pzzz-cv/RStab)

RStab is a novel framework for video stabilization that integrates 3D multi-frame fusion through volume rendering.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/ETMVSNet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**ICCV 2023**] [When Epipolar Constraint Meets Non-local Operators in Multi-View Stereo](https://arxiv.org/abs/2309.17218) \\
**Tianqi Liu**, Xinyi Ye, Weiyue Zhao, Zhiyu Pan, Min Shi, Zhiguo Cao. \\
[[Paper]](https://arxiv.org/abs/2309.17218)
[[Code]](https://github.com/TQTQliu/ET-MVSNet)

ETMVSNet uses epipolar geometric priors to constrain feature aggregation fileds, thereby efficiently inferring multi-view depths and reconstructing scenes.

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/dmvsnet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**ICCV 2023**] [Constraining Depth Map Geometry for Multi-View Stereo: A Dual-Depth Approach with Saddle-shaped Depth Cells](https://arxiv.org/abs/2307.09160) \\
Xinyi Ye, Weiyue Zhao, **Tianqi Liu**, Zihao Huang, Zhiguo Cao, Xin Li. \\
[[Paper]](https://arxiv.org/abs/2307.09160)
[[Code]](https://github.com/DIVE128/DMVSNet)

DMVSNet proposes a new perspective to consider the depth geometry of multi-view stereo and introduces a dual-depth approach to approximate the depth geometry with saddle-shaped cells.
</div>
</div>
